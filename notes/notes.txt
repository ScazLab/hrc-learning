Tweaks:
2. Adjust state redirection (currently self-loop, could do "human picks" the next state (select from valid next states))
3. Adust e-greedy selection strategy
	I notice it takes a little longer to converge (200 trials) but that's just because we're demanding high level of convergence, where all sub-optimal-paths have to be the same. Either we're waiting until Q completion, or we're waiting until a lucky block with all exploitation down pre-filled paths. Either way, good performance should arise much sooner than 200.

CORINA'S SUGGESTIONS:
x 1. Do explore vs exploit with some epslion greedy
	Currently using e = max(0.1, 1/(trial number + 1))
2. Run some numbers and collect some stats
x 3. Start tweaking reward based on some preferences. See if I can get the robot to match preference
4. Could build reward matrix to reflect future rewards outweighing current rewards

GAME PLAN:
Optimize this Q learning system and try to get it down to as few trials as possible (20). Then get it working on the robot and train it in person



Fix string to int conversion in human web interface input 

Add red button on web hi for "Error, I meant to press the red button on bot" to add -1 to matrix. Could use arbitrary state system to allow the transition. But even then that's pointless because it takes the human into states she didn't want to reach, and wastes exploration time on states that will never be revisited. If an action is so bad as to be rejected and never taken again, it should be undone by the robot on the spot.

Simple multi-armed bandit.
For comparison slide:
Log helpfulness over trials with different exploit-explore strategies. Fast initial growth is our primary goal, with long-term optimization a secondary goal 
Helpfulness = total reward for trial
1. Some naive (all explore, or first exploit then explore)
2. Epsilon-greedy (diminishing to 10%)
3. Proportional to existing rewards, maybe biasing to unexplored options. Wait that doesn't really make sense, if you're exploiting you should take the max of known options. If you're exploring try something new. No randomness in reward for a particular state so 1 explore gives perfect info. 
4. Greedy exploit with likelihood of exploring (choosing an unexplored (0)
possibility) inversely proportional to quality of current best found option. If best is a 1, 10% chance of exploiting. If best is 10, 100% chance of exploiting.

Run these all in simulation! Just tally total reward and output

Background could compare other learning methods (ANN, reinforcement) and explain why I chose Q (fast to good-enough outcomes). We're trying to make a robot mimic a human partner that figures out what its teammate wants after just a few trials, and then might gradually refine the support 

Code snippet of exploit/explore function 


CONTROLLING BAXTER
https://github.com/ScazLab/hrc_pred_supp_bhv/blob/new_task/scripts/controller.py
	(in new_task branch)
Define function called _run(self) in controller.py
take_action function has examples of how to call actions
	I don't think I need to modify take_action() though. it 
OBJECT_IDX dict has all objects 
Send to self._action what you need (arm, object id)
On that computer create a ros_devel_ws (a ros workspace). Create my own version of the package on github. Create my own catkin package in my pwn folder






I should do a bunch of trials where we check for when the Q matrix guides to the right answer (following max) for each of the strategies. Some of the high-absolute strategies might have a good inexact performance.

I should do TEN levels of preference (kind of like, really like) for the human to specify. Shows that the robot will learn more nuanced preferences

Reward   | Input
---------|------
-1/inv   | Red button
1 (ok)   | 1 green push
2 (ok+)  | 2 green pushes
3 (+++)  | 3 green pushes
4 (++++) | 4 green pushes
etc

An improvement for flexibility would be to make the T and Q matrices dictionaries instead that map a binary key (representing objects in workspace) to Q values (or to other binary keys)
That way it is totally up to the user what moves are preferred, acceptable, and unacceptable
The presentation is that the user has the option to work with the robot's action with a range of approval, or to totally reject it and demand that the robot try a different move and never do that same move in that particular situation again.
Acceptance with low approval will be subsumed by high-approval options



Let's compare four versions of exploit-explore:
1. EXPLORE FIRST. Explore every possibility until none are unexplored, then exploit.
2. EPSILON-GREEDY. At every stage, e% chance explore, 1-e% chance exploit.
3. EPSILON-DECREASING. In early trials, explore all the time, gradually diminishing to 10% explore.
4. EPSILON-PROPORTIONAL. Likelihood of exploiting best option is proportional to quality of best option found. If best is 1, 10% chance to exploit it. If best is 10 or greater, 100%. If all 0s or -1s, guaranteed explore. 

3 Variations of each:
A. explore chooses randomly from all nonnegative									- inefficient for strategy 1
B. explore chooses randomly only from unexplored; if none, choose randomly from nonnegative			- N/A for strategy 1
C. explore chooses randomly only from unexplored; if none, exploit


Results:
FIRST variants have best tail results - perfect 170s. But A takes 30 trials to get there, BC takes 15t. Linear growth from 90.
GREEDY variants grow to a shelf (135) in 2 trials, but take a while (~50t) to curve up to 163 and never go much higher than that.
DECREASING also tail at 163 but with a higher initial shelf (158) that takes longer (10t) to get to.
PROPORTIONAL tails at 168 with a rounded shelf around (5t, 152).

epdecreasing A I like. Let's try with a little more exploration (15 instead of 10 to .1)
Result: slightly higher initial shelf, but takes longer to get there. 10 is better.

******************************************************
******I WILL RUN EPSILON DECREASING A WITH BAXTER*****
******************************************************

Eventually we can uncomment the text-to-speech stuff if we want
Improvement: make it so baxter remembers when he's used both of one kind of bracket so he doesn't keep plaintively grabbing the air
Should absolutely export Q matrix	
	
PROP is Quirky because any option with value > 10 will always be exploited. Very arbitrary threshold. (issues with heavy backprop reinforcing a medium-grade option and shutting out a better one). Happened to perform well for this setup, but not generalizable


When exploring, 3 possible strategies:
	Choose randomly from all options (stupid)
	Choose randomly from all nonnegative options (not invalid)
	Choose randomly from all zero options (unexplored). Downside is the robot will never try mediocre help more than once which could blind it to big rewards down a mediocre path. But I think the high-level task itself is well-suited for assuming that mediocre actions in this state are not optimal later on. That is, human will not say, "I don't like this particualar action very much but I want you to keep doing it here because it will take us to a state I like a lot later on." Instead, they will rate that move highly. In discussion of downsides, talk about flatness of learning and how the scenario tends to flatten consideration of future rewards.


show a bunch of different preferences that work
